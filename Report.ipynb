{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collaboration and Competition\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mtensorflow 1.7.1 has requirement numpy>=1.13.3, but you'll have numpy 1.12.1 which is incompatible.\u001b[0m\r\n",
      "\u001b[31mipython 6.5.0 has requirement prompt-toolkit<2.0.0,>=1.0.15, but you'll have prompt-toolkit 3.0.8 which is incompatible.\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: TennisBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): 2\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "\n",
    "env = UnityEnvironment(file_name=\"/data/Tennis_Linux_NoVis/Tennis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of agents: 2\n",
      "Size of each action: 2\n",
      "There are 2 agents. Each observes a state with length: 24\n",
      "The state for the first agent looks like: [ 0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.          0.\n",
      "  0.          0.         -6.65278625 -1.5        -0.          0.\n",
      "  6.83172083  6.         -0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents \n",
    "num_agents = len(env_info.agents)\n",
    "print('Number of agents:', num_agents)\n",
    "\n",
    "# size of each action\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Size of each action:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "states = env_info.vector_observations\n",
    "state_size = states.shape[1]\n",
    "print('There are {} agents. Each observes a state with length: {}'.format(states.shape[0], state_size))\n",
    "print('The state for the first agent looks like:', states[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agents while they are training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total score (averaged over agents) this episode: 0.0\n",
      "Total score (averaged over agents) this episode: 0.0\n",
      "Total score (averaged over agents) this episode: 0.0\n",
      "Total score (averaged over agents) this episode: 0.0\n",
      "Total score (averaged over agents) this episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "from collections import namedtuple, deque\n",
    "\n",
    "for i in range(5):                                         # play game for 5 episodes\n",
    "    env_info = env.reset(train_mode=False)[brain_name]     # reset the environment    \n",
    "    states = env_info.vector_observations                  # get the current state (for each agent)\n",
    "    scores = np.zeros(num_agents)                          # initialize the score (for each agent)\n",
    "    while True:\n",
    "        actions = np.random.randn(num_agents, action_size) # select an action (for each agent)\n",
    "        actions = np.clip(actions, -1, 1)                  # all actions between -1 and 1\n",
    "        env_info = env.step(actions)[brain_name]           # send all actions to tne environment\n",
    "        next_states = env_info.vector_observations         # get next state (for each agent)\n",
    "        rewards = env_info.rewards                         # get reward (for each agent)\n",
    "        dones = env_info.local_done                        # see if episode finished\n",
    "        scores += env_info.rewards                         # update the score (for each agent)\n",
    "        states = next_states                               # roll over states to next time step\n",
    "        if np.any(dones):                                  # exit loop if episode finished\n",
    "            break\n",
    "    print('Total score (averaged over agents) this episode: {}'.format(np.max(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Inplement the MADDPG algorithm\n",
    "\n",
    "Multi-Agents Deep Deterministic Policy Gradient algorithm (MADDPG) is an extension of DDPG in a multi-agent environment. In the DDPG algorithm, two kinds of deep neural networks (Actor and Critic) are built for each agent. The Actor is a policy function that will be trained to map the agent's observations to its optimal action. Not like the value-based methods whose output are the probabilities in the action space, this policy-based method will directly output the expected actions. The Critic networks are Q-value functions that map the states and actions to the expected discounted return of an episode. While implementing the DDPG algorithm, a target network is built for each Actor and Critic, with the same architecture respectively. \n",
    "During training, \n",
    "\n",
    "- the agent gets an observation from the environment, then use the Actor to map from this observation to an action\n",
    "- the action added with a noised is sent to the environment for the next observation, and the reward.\n",
    "- the experience (observation, action, reward, next_observation) will be stored in a memory pool (replay buffer) \n",
    "- the next_observation become the observation of the next loop\n",
    "- a batch of experiences is a sample from the replay buffer\n",
    "- critic loss is calculated and the Critic optimizer performs a step of backpropagation\n",
    "- J value (estimated expectation of the return with current observations and actions) is calculated, and the Actor optimizer performs a backpropagation step.\n",
    "- soft update the target networks of Actor and Critic\n",
    "- move to the next loop until the episode terminates.\n",
    "\n",
    "\n",
    "In a Multi-Agent environment, the agents share their experiences during training, the critic network of each agent receives all the observations and actions in order to understand what happens to all the agents. the pseudo code from the [paper](https://arxiv.org/pdf/1706.02275.pdf) is shown below: \n",
    "\n",
    "![psedu_code](images/MADDPG_algorithm.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Architecture of the neural networks:\n",
    "\n",
    "observation size (state_size): 24\n",
    "\n",
    "numer of agents (num_agents): 2\n",
    "\n",
    "**Actor (local / target):**\n",
    "\n",
    "- layer 1: Linear(24, 256) with activation relu() \n",
    "- layer 2: Linear(256, 128) with activation relu()\n",
    "- layer 3: Linear(128, 2) with activation tanh()\n",
    "\n",
    "**Critic (local / target):**\n",
    "\n",
    "- layer 1: Linear(24\\*2, 256) with activation relu() \n",
    "- layer 2: Linear(256 + 2\\*2, 128) with activation relu()\n",
    "- layer 3: Linear(128, 1), no activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "import random\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hiddens=(256,128)):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hiddens[0])\n",
    "        self.fc2 = nn.Linear(hiddens[0], hiddens[1])\n",
    "        self.fc3 = nn.Linear(hiddens[1], action_size)\n",
    "        # initialize the weights of the last fc layer\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = torch.tanh(self.fc3(x))\n",
    "        return x\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size, hiddens=(256,128)):\n",
    "        super(Critic, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, hiddens[0])\n",
    "        self.fc2 = nn.Linear(hiddens[0] + action_size, hiddens[1])\n",
    "        self.fc3 = nn.Linear(hiddens[1], 1)\n",
    "        self.fc3.weight.data.uniform_(-3e-3, 3e-3)\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        hidden_0_out = F.relu(self.fc1(states))\n",
    "        # actions are input in the second layer of the Critic network\n",
    "        hidden_1_in = torch.cat((hidden_0_out, actions), dim=1)\n",
    "        hidden_1_out = F.relu(self.fc2(hidden_1_in))\n",
    "        return self.fc3(hidden_1_out)\n",
    "\n",
    "class OUNoise:\n",
    "    def __init__(self, size, mu=0.0, theta=0.2, sigma=0.1):\n",
    "        self.size = size\n",
    "        self.mu = mu * np.ones(size)\n",
    "        self.theta = theta\n",
    "        self.sigma = sigma\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = copy.copy(self.mu)\n",
    "\n",
    "    def sample(self):\n",
    "        s = self.state\n",
    "        ds = self.theta * (self.mu - s) + self.sigma * np.random.standard_normal(self.size)\n",
    "        self.state = s + ds\n",
    "        return self.state\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, buffer_size, seed):\n",
    "        self.experience = namedtuple('Experience',field_names=('states', 'actions', 'rewards', 'next_states', 'dones'))\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.seed = random.seed(seed)\n",
    "\n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        self.memory.append(self.experience(state, action, reward, next_state, done))\n",
    "\n",
    "    def sample(self, device, batch_size):\n",
    "        memory_sample = random.sample(self.memory, batch_size)\n",
    "        states = torch.from_numpy(np.stack([experience.states for experience in memory_sample])).float().to(device)\n",
    "        actions = torch.from_numpy(np.stack([experience.actions for experience in memory_sample])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.stack([experience.rewards for experience in memory_sample])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.stack([experience.next_states for experience in memory_sample])).float().to(device)\n",
    "        dones = torch.from_numpy(np.stack([experience.dones for experience in memory_sample]).astype(np.uint8)).float().to(device)\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "class DDPG_Agent:\n",
    "    # initialize the DDPG_agent\n",
    "    def __init__(self, device, num_agents, state_size, action_size, lr_actor, lr_critic):\n",
    "        self.device = device\n",
    "        # actor and critic (local/target) networks and optimizers\n",
    "        self.actor_local = Actor(state_size, action_size).to(device)\n",
    "        self.actor_target = Actor(state_size, action_size).to(device)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor_local.parameters(), lr=lr_actor)\n",
    "        # input sizes of critic network are state_size * num_agents and action_size * num_agents \n",
    "        self.critic_local = Critic(state_size * num_agents, action_size * num_agents).to(device)\n",
    "        self.critic_target = Critic(state_size * num_agents, action_size * num_agents).to(device)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic_local.parameters(), lr=lr_critic)\n",
    "\n",
    "class MADDPG:\n",
    "    # initialize the MADDPG\n",
    "    def __init__(self, device, num_agents, state_size, action_size, lr_actor, lr_critic, buffer_size, seed, batch_size, gamma, tau):\n",
    "        self.device = device\n",
    "        self.num_agents = num_agents\n",
    "        self.agents = [DDPG_Agent(device, num_agents, state_size, action_size, lr_actor, lr_critic) for i in range(self.num_agents)]\n",
    "        self.replaybuffer = ReplayBuffer(buffer_size, seed)\n",
    "        self.noise = OUNoise(action_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau    \n",
    "        \n",
    "    def reset_noise(self):\n",
    "        self.noise.reset()\n",
    "    \n",
    "    def act(self, states, add_noise=True):\n",
    "        states = torch.from_numpy(states).float().to(self.device)\n",
    "\n",
    "        noise = np.zeros(self.noise.size)\n",
    "        if add_noise:\n",
    "            noise = self.noise.sample()\n",
    "        \n",
    "        actions = []  \n",
    "        for i in range(self.num_agents):\n",
    "            action = self.agents[i].actor_local(states[i]).cpu().data.numpy() + noise\n",
    "            action = np.clip(action, -1.0, 1.0)\n",
    "            actions.append(action)\n",
    "        return np.array(actions)        \n",
    "        \n",
    "    def store(self, states, actions, rewards, next_states, dones):\n",
    "        self.replaybuffer.add(states, actions, rewards, next_states, dones)\n",
    "\n",
    "    def learn(self):\n",
    "        for i , agent in enumerate(self.agents):\n",
    "            states, actions, rewards, next_states, dones = self.replaybuffer.sample(self.device, self.batch_size)        \n",
    "            # calculate critic_loss and make a step of optimizing the parameters of critic_local\n",
    "            states_combined = states.view(self.batch_size, -1)\n",
    "            actions_combined = actions.view(self.batch_size, -1)\n",
    "            next_states_combined = next_states.view(self.batch_size, -1)\n",
    "            next_actions = [self.agents[i].actor_target(next_states[:,i]) for i in range(self.num_agents)]\n",
    "            next_actions_combined = torch.stack(next_actions, dim=1).view(self.batch_size, -1)\n",
    "            q_next = agent.critic_target(next_states_combined, next_actions_combined)\n",
    "            q_target = rewards[:,i].unsqueeze(1) + (1 - dones[:,i].unsqueeze(1)) * self.gamma * q_next\n",
    "            q = agent.critic_local(states_combined, actions_combined)\n",
    "            critic_loss = torch.nn.functional.mse_loss(q, q_target)\n",
    "            agent.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            agent.critic_optimizer.step()\n",
    "            # calculate J and make a step of optimizing the parameters of actor_local\n",
    "            agent.actor_optimizer.zero_grad()\n",
    "            actions_pred = [self.agents[i].actor_local(states[:,i]) for i in range(self.num_agents)]\n",
    "            actions_pred_combined = torch.stack(actions_pred, dim=1).view(self.batch_size, -1)\n",
    "            J = -agent.critic_local(states_combined, actions_pred_combined).mean()\n",
    "            J.backward()\n",
    "            agent.actor_optimizer.step()\n",
    "\n",
    "            # soft_update the actor_target and critic_target network\n",
    "            self.soft_update(agent.actor_local.parameters(), agent.actor_target.parameters())\n",
    "            self.soft_update(agent.critic_local.parameters(), agent.critic_target.parameters())        \n",
    "\n",
    "    def soft_update(self, local_parameters, target_parameters):\n",
    "        for local_param, target_param in zip(local_parameters, target_parameters):\n",
    "            target_param.data.copy_(target_param.data * (1.0 - self.tau) + local_param.data * self.tau)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform the Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment solved at episode: 4060! Average Score = 0.500600.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7efc3be8be10>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "\n",
    "# Hyper-parameters\n",
    "lr_actor = 1e-4\n",
    "lr_critic = 5e-4\n",
    "buffer_size = int(1e+5)\n",
    "seed = 123\n",
    "batch_size = 256\n",
    "gamma = 0.99\n",
    "tau = 1e-3\n",
    "\n",
    "# An instance of MADDPG \n",
    "maddpg = MADDPG(device, num_agents, state_size, action_size, lr_actor, lr_critic, buffer_size, seed, batch_size, gamma, tau)\n",
    "\n",
    "num_episode = 5000\n",
    "\n",
    "scores_all = []\n",
    "scores_window = deque(maxlen=100)\n",
    "scores_average = []\n",
    "\n",
    "from workspace_utils import active_session\n",
    "\n",
    "with active_session():\n",
    "    for episode_i in range(num_episode):\n",
    "        maddpg.reset_noise()\n",
    "        brain_info = env.reset(train_mode=True)[brain_name]\n",
    "        states = brain_info.vector_observations\n",
    "        score_episode = np.zeros(len(states))\n",
    "        while True:\n",
    "            actions = maddpg.act(states, add_noise=True)\n",
    "            brain_info = env.step(actions)[brain_name]\n",
    "            next_states = brain_info.vector_observations\n",
    "            rewards = brain_info.rewards\n",
    "            dones = brain_info.local_done\n",
    "            score_episode += rewards\n",
    "            maddpg.store(states, actions, rewards, next_states, dones)\n",
    "            if len(maddpg.replaybuffer) > batch_size:\n",
    "                maddpg.learn()\n",
    "            if np.any(dones):\n",
    "                break\n",
    "            states = next_states\n",
    "\n",
    "        scores_all.append(np.max(score_episode))\n",
    "        scores_window.append(np.max(score_episode))\n",
    "\n",
    "        if episode_i < 99:\n",
    "            print(\"\\rEpisode: {}\\tScore: {:4f}\".format(episode_i, scores_all[episode_i]), end=\"\")\n",
    "        else:\n",
    "            scores_average.append(np.mean(scores_window))\n",
    "            print(\"\\rEpisode: {}\\tScore: {:4f}\\tAverage Score: {:4f}\".format(episode_i, scores_all[episode_i], scores_average[episode_i - 99]), end=\"\")\n",
    "            if scores_average[episode_i - 99] > 0.5:\n",
    "                print(\"\\rEnvironment solved at episode: {}! Average Score = {:4f}.\".format(episode_i, scores_average[episode_i - 99]))\n",
    "                break\n",
    "\n",
    "for i in range(num_agents):\n",
    "    torch.save(maddpg.agents[i].actor_local.state_dict(), 'agent_{}_actor_checkpoint.pth'.format(i))\n",
    "    torch.save(maddpg.agents[i].critic_local.state_dict(), 'agent_{}_critic_checkpoint.pth'.format(i))\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "fig = plt.figure()\n",
    "plt.plot(np.arange(len(scores_all)), scores_all, label=\"score\")\n",
    "plt.plot(np.arange(len(scores_average))+99, scores_average, label=\"average_score\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Episode #\")\n",
    "plt.ylabel(\"Score\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Future ideas\n",
    "\n",
    "Even the environment is solved, the training are not stable. With the same hyper parameters, sometimes it failed, sometimes it solved the problem. It is hard to tune the hyper parameters. The latest ones are inspired by [vivekthota16](https://github.com/vivekthota16/Project-Collaboration-and-Competition-Udacity-Deep-Reinforcement-Learning), especially the weight initialization of the last layers of the Actor and Critic. So the next work would be to try more hyperparameters to better understand the driving parameters for success.\n",
    "Also, other technics like Prioritized Experience Replay is an option to try."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
